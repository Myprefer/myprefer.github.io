{"title":"ML 现代卷积神经网络","uid":"1084723f5f84c83e372c0f57ae570bd1","slug":"ML-现代卷积神经网络","date":"2024-08-20T12:35:28.000Z","updated":"2024-08-20T14:54:45.235Z","comments":true,"path":"api/articles/ML-现代卷积神经网络.json","keywords":null,"cover":[],"content":"<h2 id=\"现代卷积神经网络\"><a href=\"#现代卷积神经网络\" class=\"headerlink\" title=\"现代卷积神经网络\"></a>现代卷积神经网络</h2><h3 id=\"LeNet\"><a href=\"#LeNet\" class=\"headerlink\" title=\"LeNet\"></a>LeNet</h3><p><img src=\"https://img-blog.csdnimg.cn/direct/3cc9b3544c3a4db1b652406be4481008.png\"></p>\n<h3 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h3><p>AlexNet和LeNet的设计理念和架构非常相似，但也存在显著差异。本质上来说，AlexNet是改进后的更深更大的LeNet</p>\n<p>主要改进:</p>\n<ul>\n<li><p>全连接层的隐藏层后加入了丢弃层</p>\n</li>\n<li><p>sigmoid –&gt; ReLU (减缓梯度消失)</p>\n</li>\n<li><p>平均池化 –&gt; 最大池化</p>\n</li>\n<li><p>进行了数据增强</p>\n</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240819205903.png\"></p>\n<p>LeNet与AlexNet</p>\n<h3 id=\"VGG-使用块的网络\"><a href=\"#VGG-使用块的网络\" class=\"headerlink\" title=\"VGG(使用块的网络)\"></a>VGG(使用块的网络)</h3><p>VGG块：可看作更大更深的AlexNet</p>\n<p>VGG使用可重复的卷积块来构建深度卷积神经网络</p>\n<p>VGG架构：</p>\n<ul>\n<li><p>替换掉AlexNet整个卷积的架构，形成n个VGG块串在一起, 多个VGG块后接全连接层</p>\n</li>\n<li><p>不同的卷积块个数和超参数可以达到不同复杂度的变种(VGG16, VGG19)</p>\n</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240819222413.png\"></p>\n<p><strong>vgg块的实现</strong></p>\n<div class=\"language-py\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">py</span><pre class=\"shiki material-theme-palenight\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #C792EA\">def</span><span style=\"color: #BABED8\"> </span><span style=\"color: #82AAFF\">vgg_block</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #BABED8; font-style: italic\">num_convs</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #BABED8\"> </span><span style=\"color: #BABED8; font-style: italic\">in_channels</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #BABED8\"> </span><span style=\"color: #BABED8; font-style: italic\">out_channels</span><span style=\"color: #89DDFF\">):</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">    layers </span><span style=\"color: #89DDFF\">=</span><span style=\"color: #BABED8\"> </span><span style=\"color: #89DDFF\">[]</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">    </span><span style=\"color: #89DDFF; font-style: italic\">for</span><span style=\"color: #BABED8\"> _ </span><span style=\"color: #89DDFF; font-style: italic\">in</span><span style=\"color: #BABED8\"> </span><span style=\"color: #82AAFF\">range</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #82AAFF\">num_convs</span><span style=\"color: #89DDFF\">):</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">        layers</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">append</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #82AAFF\">nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">Conv2d</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #82AAFF\">in_channels</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #82AAFF\"> out_channels</span><span style=\"color: #89DDFF\">,</span></span>\n<span class=\"line\"><span style=\"color: #82AAFF\">                                </span><span style=\"color: #BABED8; font-style: italic\">kernel_size</span><span style=\"color: #89DDFF\">=</span><span style=\"color: #F78C6C\">3</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #82AAFF\"> </span><span style=\"color: #BABED8; font-style: italic\">padding</span><span style=\"color: #89DDFF\">=</span><span style=\"color: #F78C6C\">1</span><span style=\"color: #89DDFF\">))</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">        layers</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">append</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #82AAFF\">nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">ReLU</span><span style=\"color: #89DDFF\">())</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">        in_channels </span><span style=\"color: #89DDFF\">=</span><span style=\"color: #BABED8\"> out_channels</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">    layers</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">append</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #82AAFF\">nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">MaxPool2d</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #BABED8; font-style: italic\">kernel_size</span><span style=\"color: #89DDFF\">=</span><span style=\"color: #F78C6C\">2</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #BABED8; font-style: italic\">stride</span><span style=\"color: #89DDFF\">=</span><span style=\"color: #F78C6C\">2</span><span style=\"color: #89DDFF\">))</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">    </span><span style=\"color: #89DDFF; font-style: italic\">return</span><span style=\"color: #BABED8\"> nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">Sequential</span><span style=\"color: #89DDFF\">(*</span><span style=\"color: #82AAFF\">layers</span><span style=\"color: #89DDFF\">)</span></span></code></pre></div><p><strong>vgg网络的实现</strong></p>\n<div class=\"language-python\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">python</span><pre class=\"shiki material-theme-palenight\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #BABED8\">conv_arch </span><span style=\"color: #89DDFF\">=</span><span style=\"color: #BABED8\"> </span><span style=\"color: #89DDFF\">((</span><span style=\"color: #F78C6C\">1</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #BABED8\"> </span><span style=\"color: #F78C6C\">64</span><span style=\"color: #89DDFF\">),</span><span style=\"color: #BABED8\"> </span><span style=\"color: #89DDFF\">(</span><span style=\"color: #F78C6C\">1</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #BABED8\"> </span><span style=\"color: #F78C6C\">128</span><span style=\"color: #89DDFF\">),</span><span style=\"color: #BABED8\"> </span><span style=\"color: #89DDFF\">(</span><span style=\"color: #F78C6C\">2</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #BABED8\"> </span><span style=\"color: #F78C6C\">256</span><span style=\"color: #89DDFF\">),</span><span style=\"color: #BABED8\"> </span><span style=\"color: #89DDFF\">(</span><span style=\"color: #F78C6C\">2</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #BABED8\"> </span><span style=\"color: #F78C6C\">512</span><span style=\"color: #89DDFF\">),</span><span style=\"color: #BABED8\"> </span><span style=\"color: #89DDFF\">(</span><span style=\"color: #F78C6C\">2</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #BABED8\"> </span><span style=\"color: #F78C6C\">512</span><span style=\"color: #89DDFF\">))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #C792EA\">def</span><span style=\"color: #BABED8\"> </span><span style=\"color: #82AAFF\">vgg</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #BABED8; font-style: italic\">conv_arch</span><span style=\"color: #89DDFF\">):</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">    conv_blks </span><span style=\"color: #89DDFF\">=</span><span style=\"color: #BABED8\"> </span><span style=\"color: #89DDFF\">[]</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">    in_channels </span><span style=\"color: #89DDFF\">=</span><span style=\"color: #BABED8\"> </span><span style=\"color: #F78C6C\">1</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">    </span><span style=\"color: #676E95; font-style: italic\"># 卷积层部分</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">    </span><span style=\"color: #89DDFF; font-style: italic\">for</span><span style=\"color: #BABED8\"> </span><span style=\"color: #89DDFF\">(</span><span style=\"color: #BABED8\">num_convs</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #BABED8\"> out_channels</span><span style=\"color: #89DDFF\">)</span><span style=\"color: #BABED8\"> </span><span style=\"color: #89DDFF; font-style: italic\">in</span><span style=\"color: #BABED8\"> conv_arch</span><span style=\"color: #89DDFF\">:</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">        conv_blks</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">append</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #82AAFF\">vgg_block</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #82AAFF\">num_convs</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #82AAFF\"> in_channels</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #82AAFF\"> out_channels</span><span style=\"color: #89DDFF\">))</span></span>\n<span class=\"line\"><span style=\"color: #BABED8\">        in_channels </span><span style=\"color: #89DDFF\">=</span><span style=\"color: #BABED8\"> out_channels</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #BABED8\">    </span><span style=\"color: #89DDFF; font-style: italic\">return</span><span style=\"color: #BABED8\"> nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">Sequential</span><span style=\"color: #89DDFF\">(</span></span>\n<span class=\"line\"><span style=\"color: #82AAFF\">        </span><span style=\"color: #89DDFF\">*</span><span style=\"color: #82AAFF\">conv_blks</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #82AAFF\"> nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">Flatten</span><span style=\"color: #89DDFF\">(),</span></span>\n<span class=\"line\"><span style=\"color: #82AAFF\">        </span><span style=\"color: #676E95; font-style: italic\"># 全连接层部分</span></span>\n<span class=\"line\"><span style=\"color: #82AAFF\">        nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">Linear</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #82AAFF\">out_channels </span><span style=\"color: #89DDFF\">*</span><span style=\"color: #82AAFF\"> </span><span style=\"color: #F78C6C\">7</span><span style=\"color: #82AAFF\"> </span><span style=\"color: #89DDFF\">*</span><span style=\"color: #82AAFF\"> </span><span style=\"color: #F78C6C\">7</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #82AAFF\"> </span><span style=\"color: #F78C6C\">4096</span><span style=\"color: #89DDFF\">),</span><span style=\"color: #82AAFF\"> nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">ReLU</span><span style=\"color: #89DDFF\">(),</span><span style=\"color: #82AAFF\"> nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">Dropout</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #F78C6C\">0.5</span><span style=\"color: #89DDFF\">),</span></span>\n<span class=\"line\"><span style=\"color: #82AAFF\">        nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">Linear</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #F78C6C\">4096</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #82AAFF\"> </span><span style=\"color: #F78C6C\">4096</span><span style=\"color: #89DDFF\">),</span><span style=\"color: #82AAFF\"> nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">ReLU</span><span style=\"color: #89DDFF\">(),</span><span style=\"color: #82AAFF\"> nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">Dropout</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #F78C6C\">0.5</span><span style=\"color: #89DDFF\">),</span></span>\n<span class=\"line\"><span style=\"color: #82AAFF\">        nn</span><span style=\"color: #89DDFF\">.</span><span style=\"color: #82AAFF\">Linear</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #F78C6C\">4096</span><span style=\"color: #89DDFF\">,</span><span style=\"color: #82AAFF\"> </span><span style=\"color: #F78C6C\">10</span><span style=\"color: #89DDFF\">))</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color: #BABED8\">net </span><span style=\"color: #89DDFF\">=</span><span style=\"color: #BABED8\"> </span><span style=\"color: #82AAFF\">vgg</span><span style=\"color: #89DDFF\">(</span><span style=\"color: #82AAFF\">conv_arch</span><span style=\"color: #89DDFF\">)</span></span></code></pre></div><h3 id=\"NiN-网络中的网络\"><a href=\"#NiN-网络中的网络\" class=\"headerlink\" title=\"NiN(网络中的网络)\"></a>NiN(网络中的网络)</h3><p><strong>全连接层的问题</strong></p>\n<ul>\n<li>卷积层需要较少的参数</li>\n<li>但卷积层后的第一个全连接层的参数非常多<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240820183147.png\"></li>\n</ul>\n<p><strong>NiN块</strong></p>\n<ul>\n<li>一个卷积层跟两个1x1卷积层(当作全连接层)</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240820180602.png\"></p>\n<p><strong>NiN架构</strong></p>\n<ul>\n<li>无全连接层</li>\n<li>交替使用NiN块和步幅为2的最大池化层, 逐步减小高宽和增大通道数</li>\n<li>最后使用全局平均池化层得到输出<ul>\n<li>其输入通道数是类别数</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240820180825.png\"></p>\n<h3 id=\"GoogLeNet-含并行连结的网络\"><a href=\"#GoogLeNet-含并行连结的网络\" class=\"headerlink\" title=\"GoogLeNet(含并行连结的网络)\"></a>GoogLeNet(含并行连结的网络)</h3><p><strong>Inception块</strong></p>\n<p>四个路径从不同层面抽取信息, 然后再输出通道维合并</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240820201452.png\"></p>\n<ul>\n<li><p>白色的卷积用来改变通道数，蓝色的卷积用来抽取信息。</p>\n</li>\n<li><p>最左边一条1X1卷积是用来抽取通道信息，其他的3X3卷积用来抽取空间信息。</p>\n</li>\n<li><p>通过降低通道数来控制模型复杂度, 每条路上的通道数可能不同</p>\n</li>\n<li><p>它的一个主要优点是模型参数小, 计算复杂度低</p>\n</li>\n</ul>\n<p><strong>GoogLeNet结构</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240820202053.png\"></p>\n<h3 id=\"ResNet-残差网络\"><a href=\"#ResNet-残差网络\" class=\"headerlink\" title=\"ResNet (残差网络)\"></a>ResNet (残差网络)</h3><p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240820221137.png\"></p>\n<p>对于非嵌套函数类, 较复杂(图中较大区域)的函数类不能保证更加逼近真实, 而嵌套函数类能保证更加复杂的模型起码不会更差</p>\n<p><strong>残差块</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240820223510.png\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240820223530.png\"></p>\n<p>残差块使得很深的网络更加容易训练, 甚至可以训练一千层的网络</p>\n<p><strong>ResNet块</strong></p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240820223542.png\"></p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240820223553.png\"></p>\n<hr>\n<p><strong>ResNet如何解决梯度消失</strong></p>\n<p>ResNet的操作相当于<br>$$<br>y’’ &#x3D; g(f(x))变为y’’&#x3D;f(x) + g(f(x))<br>$$<br>这样其导数<br>$$<br>\\frac{∂y’’}{∂w}&#x3D;\\frac{∂y}{∂w}+\\frac{∂y’}{∂w}<br>$$<br>即使<code>y&#39;</code>的导数非常小, 加上y的导数后也不会太小</p>\n","feature":true,"text":"现代卷积神经网络LeNet AlexNetAlexNet和LeNet的设计理念和架构非常相似，但也存在显著差异。本质上来说，AlexNet是改进后的更深更大的L...","permalink":"/post/ML-现代卷积神经网络","photos":[],"count_time":{"symbolsCount":"1.9k","symbolsTime":"2 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">现代卷积神经网络</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#LeNet\"><span class=\"toc-text\">LeNet</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#AlexNet\"><span class=\"toc-text\">AlexNet</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#VGG-%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">VGG(使用块的网络)</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#NiN-%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">NiN(网络中的网络)</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#GoogLeNet-%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">GoogLeNet(含并行连结的网络)</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#ResNet-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">ResNet (残差网络)</span></a></li></ol></li></ol>","author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"加入择栖工作室谢谢喵~      学习机器学习谢谢喵~","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"ML 理论基础","uid":"23f2ddd5bdc180cd3f2c05e6b37a137d","slug":"ML-理论基础","date":"2024-09-07T16:31:52.000Z","updated":"2024-09-07T16:32:29.053Z","comments":true,"path":"api/articles/ML-理论基础.json","keywords":null,"cover":[],"text":"理论基础讨论前向传播和反向传播的流程？前向传播 在输入层输入特征向量, 传递到隐藏层 在隐藏层不断计算出每一层神经元得到的结果 通过激活函数得到的输出 将输出传...","permalink":"/post/ML-理论基础","photos":[],"count_time":{"symbolsCount":"6.2k","symbolsTime":"6 mins."},"categories":[],"tags":[],"author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"加入择栖工作室谢谢喵~      学习机器学习谢谢喵~","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},"next_post":{"title":"ML 卷积神经网络","uid":"f7de8d8b747477c8ddf056b54ebdb44d","slug":"ML-卷积神经网络","date":"2024-08-13T15:50:25.000Z","updated":"2024-08-20T13:32:45.609Z","comments":true,"path":"api/articles/ML-卷积神经网络.json","keywords":null,"cover":[],"text":"卷积神经网络卷积层 在卷积层中, 使用卷积核在输入图像上滑动, 并计算卷积核与局部图像块之间的点积, 生成特征图, 这一过程称为卷积。 如图, 3x3的卷积核在...","permalink":"/post/ML-卷积神经网络","photos":[],"count_time":{"symbolsCount":"1.7k","symbolsTime":"2 mins."},"categories":[],"tags":[],"author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"加入择栖工作室谢谢喵~      学习机器学习谢谢喵~","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}