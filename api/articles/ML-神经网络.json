{"title":"ML 神经网络","uid":"13909e268f0450d522608f992a88d1f0","slug":"ML-神经网络","date":"2024-08-03T08:43:22.000Z","updated":"2024-08-11T11:48:48.305Z","comments":true,"path":"api/articles/ML-神经网络.json","keywords":null,"cover":[],"content":"<h2 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h2><p>用于识别复杂模式和数据中的特征。是机器学习和深度学习中的基础工具，特别适用于处理图像、语音和文本等非结构化数据。</p>\n<h3 id=\"层（Layer）\"><a href=\"#层（Layer）\" class=\"headerlink\" title=\"层（Layer）\"></a>层（Layer）</h3><p>神经元按层组织，每一层的神经元与上一层的神经元相连。通常，神经网络包括输入层、隐藏层和输出层。</p>\n<ul>\n<li><p><strong>输入层</strong>：接收外部数据作为输入。</p>\n</li>\n<li><p><strong>隐藏层</strong>：通过多个隐藏层进行复杂的特征提取和模式识别。</p>\n</li>\n<li><p><strong>输出层</strong>：输出最终的预测结果或分类结果。</p>\n</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408031747355.png\"></p>\n<p>每一层的输出成为下一层的输入 </p>\n<h3 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h3><p>在多层神经网络中, 上层节点的输出和下层节点的输入之间存在一个函数关系, 这个函数称为激活函数</p>\n<h4 id=\"激活函数的作用\"><a href=\"#激活函数的作用\" class=\"headerlink\" title=\"激活函数的作用\"></a>激活函数的作用</h4><p>如果不使用激活函数(或者是只使用线性激活函数), 那么每一层节点的输入都是上层输出的线性变换, 这种情况下, 无论神经网络有多少层, 输出都只会是输入的线性组合, 网络的效果与没有隐藏层时的效果差不多, 网络的逼近能力就非常有限</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408041616976.png\"></p>\n<p>如果引入非线性函数作为激活函数, 那么网络的表达能力就会非常强大, 几乎可以逼近任何函数</p>\n<h4 id=\"激活函数的选择\"><a href=\"#激活函数的选择\" class=\"headerlink\" title=\"激活函数的选择\"></a>激活函数的选择</h4><ul>\n<li><p><strong>sigomoid</strong><br>$$<br>\\sigma(x) &#x3D; \\frac{1}{1 + e^{-x}}<br>$$<br><strong>适用场景</strong>：用于输出层处理<strong>二分类</strong>问题。</p>\n</li>\n<li><p><strong>ReLU</strong><br>$$<br>ReLU(x)&#x3D;max(0,x)<br>$$<br><strong>适用场景</strong>：</p>\n<ul>\n<li>广泛用于<strong>隐藏层</strong>，特别是在深度神经网络中。</li>\n<li>用于<strong>回归问题</strong>的输出层</li>\n</ul>\n</li>\n<li><p><strong>Softmax</strong><br>$$<br>\\text{Softmax}(x_i) &#x3D; \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}<br>$$<br><strong>适用场景</strong>：用于<strong>多分类</strong>问题的输出层。</p>\n</li>\n</ul>\n<h3 id=\"前向传播\"><a href=\"#前向传播\" class=\"headerlink\" title=\"前向传播\"></a>前向传播</h3><p>神经网络前向传播从输入层到输出层：前向传播就是从输入层开始，经过一层层的Layer，不断计算每一层的神经网络得到的结果及通过激活函数的本层输出结果，最后得到输出的过程。</p>\n<p><strong>输入层-&gt;隐藏层1</strong></p>\n<p><strong>隐藏层1-&gt;隐藏层2</strong></p>\n<p><strong>…</strong></p>\n<p><strong>隐藏层n-&gt;输出层</strong></p>\n<h3 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h3><ul>\n<li>前向传播计算出了输出值（也即预测值），就可以根据输出值与目标值的差别来计算loss</li>\n<li>反向传播使根据损失函数计算损失函数相对于每个参数的梯度，来调整网络的参数</li>\n</ul>\n<h4 id=\"反向传播的步骤\"><a href=\"#反向传播的步骤\" class=\"headerlink\" title=\"反向传播的步骤\"></a>反向传播的步骤</h4><ul>\n<li>计算输出层的误差</li>\n<li>反向传播误差<ul>\n<li>对于每个隐藏层，计算误差</li>\n<li>从输出层开始，一层层反向计算，直到输入层</li>\n</ul>\n</li>\n<li>梯度计算</li>\n<li>参数更新</li>\n</ul>\n","text":"神经网络用于识别复杂模式和数据中的特征。是机器学习和深度学习中的基础工具，特别适用于处理图像、语音和文本等非结构化数据。 层（Layer）神经元按层组织，每一层...","permalink":"/post/ML-神经网络","photos":[],"count_time":{"symbolsCount":946,"symbolsTime":"1 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">神经网络</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%B1%82%EF%BC%88Layer%EF%BC%89\"><span class=\"toc-text\">层（Layer）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">激活函数</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8\"><span class=\"toc-text\">激活函数的作用</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9\"><span class=\"toc-text\">激活函数的选择</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD\"><span class=\"toc-text\">前向传播</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD\"><span class=\"toc-text\">反向传播</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E6%AD%A5%E9%AA%A4\"><span class=\"toc-text\">反向传播的步骤</span></a></li></ol></li></ol></li></ol>","author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"An ordinary learner","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"ML 多分类问题","uid":"82fd8a20d046045f9dc01ea9c9906ce2","slug":"ML-多分类问题","date":"2024-08-04T09:32:35.000Z","updated":"2024-08-11T06:51:29.777Z","comments":true,"path":"api/articles/ML-多分类问题.json","keywords":null,"cover":[],"text":"多分类问题从回归到分类回归 单连续数值输出 跟真实值的区别作为损失 分类 通常多个输出 输出i时预测为第i类的置信度(概率) softmax 回归算法Softm...","permalink":"/post/ML-多分类问题","photos":[],"count_time":{"symbolsCount":348,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"An ordinary learner","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"ML 正则化","uid":"31038f1c745d94422e3236cfc3b2ffbe","slug":"ML-正则化","date":"2024-08-03T06:40:12.000Z","updated":"2024-08-13T12:58:48.962Z","comments":true,"path":"api/articles/ML-正则化.json","keywords":null,"cover":[],"text":"正则化在机器学习模型中，正则化通过在模型的损失函数中添加一个正则项来实现。这些惩罚项通常会限制模型的复杂度，使模型更加简单，从而更好地泛化到新数据。 在代价函数...","permalink":"/post/ML-正则化","photos":[],"count_time":{"symbolsCount":610,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"An ordinary learner","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}