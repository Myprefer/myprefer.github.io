{"title":"ML 卷积神经网络","uid":"f7de8d8b747477c8ddf056b54ebdb44d","slug":"ML-卷积神经网络","date":"2024-08-13T15:50:25.000Z","updated":"2024-08-20T13:32:45.609Z","comments":true,"path":"api/articles/ML-卷积神经网络.json","keywords":null,"cover":[],"content":"<h2 id=\"卷积神经网络\"><a href=\"#卷积神经网络\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h2><h3 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h3><p> 在卷积层中, 使用卷积核在输入图像上滑动, 并计算卷积核与局部图像块之间的点积, 生成特征图, 这一过程称为<strong>卷积</strong>。</p>\n<p>如图, 3x3的卷积核在4x4的图像上滑动, 生成2x2的特征图</p>\n<p><img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\"></p>\n<p>通过这种方式，卷积层能够捕捉图像中的边缘、纹理等局部特征。</p>\n<ul>\n<li>例如，一个简单的卷积核, 用于检测水平边缘:</li>\n</ul>\n<p>$$<br>K &#x3D; \\begin{pmatrix} -1 &amp; -1 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\end{pmatrix}<br>$$</p>\n<ul>\n<li>图像上某一局部区域的像素值在卷积操作中，如果存在水平方向上的亮度变化（例如，从黑色像素到白色像素），卷积的结果就会是一个较大的值，表示该区域存在水平边缘</li>\n</ul>\n<p>卷积核和偏移(b)是可学习的参数</p>\n<p>卷积核的大小是超参数</p>\n<h4 id=\"填充和步幅\"><a href=\"#填充和步幅\" class=\"headerlink\" title=\"填充和步幅\"></a>填充和步幅</h4><p>(Padding, strides)<br><img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif\" alt=\"img\"></p>\n<ul>\n<li>填充和步幅是卷积层的超参数</li>\n<li>填充正在输入周围添加额外的行&#x2F;列, 来控制输出的减少量</li>\n<li>步幅是每次滑动卷积核的步长, 可以成倍的减少输出的形状</li>\n</ul>\n<h4 id=\"通道\"><a href=\"#通道\" class=\"headerlink\" title=\"通道\"></a>通道</h4><p>神经网络中另外一个重要的超参数是<strong>通道数</strong></p>\n<p><strong>多个输入通道</strong></p>\n<ul>\n<li><p>彩色图片一般有RGB三个通道<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408141329213.png\"></p>\n</li>\n<li><p>每个通道都有一个卷积核, 结果是所有通道卷积结果的和<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408141330539.png\"></p>\n</li>\n</ul>\n<p><strong>多个输出通道</strong></p>\n<ul>\n<li><p>为什么要有多个输出通道？因为不管有多少个输入通道只会得到单输出通道的话是不够的</p>\n</li>\n<li><p>如果对每一个输出通道有一个三维的卷积核，这个卷积核会输出自己的通道（就相当于在三维的基础上又加了一维 i ，这一维表示输出的通道数）</p>\n</li>\n<li><p>可以有多个三维卷积核, 每个核生成一个输出通道</p>\n</li>\n<li><p>每个输出通道可以识别特定模式</p>\n</li>\n<li><p>输入通道核识别并组合输入中的模式</p>\n</li>\n</ul>\n<h4 id=\"1x1卷积层\"><a href=\"#1x1卷积层\" class=\"headerlink\" title=\"1x1卷积层\"></a>1x1卷积层</h4><ul>\n<li><p>卷积核的高和宽都等于1，意味着它不会识别空间信息，因为他每次只看一个空间像素所以不会去识别通道中的空间信息</p>\n</li>\n<li><p>1x1卷积不识别空间模式, 只是融合不同通道的信息</p>\n</li>\n<li><p>相当于输入形状为n_h*n_w*c, 权重为c_o*c_i的全连接层<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408141427758.png\"></p>\n</li>\n</ul>\n<p><strong>作用</strong></p>\n<ul>\n<li><p>融合不同通道的信息, 可以认为是不做空间的匹配, 只是在输入层直接做输入通道和输出通道的融合, 等价于将整个输入拉成一个向量, 通道数等于特征的数量, 卷积核就相当于一个全连接层</p>\n</li>\n<li><p>1x1卷积可以在不改变输入特征图空间分辨率的情况下, 改变其通道数, 卷积层的输出的通道数由卷积核的个数决定, 通过设置1x1卷积核的数量, 可以做到升维&#x2F;降维</p>\n</li>\n</ul>\n<h3 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h3><h4 id=\"二维最大池化\"><a href=\"#二维最大池化\" class=\"headerlink\" title=\"二维最大池化\"></a>二维最大池化</h4><ul>\n<li>返回滑动窗口的<strong>最大值</strong><br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408141504312.png\"></li>\n</ul>\n<p>例如2x2最大池化将卷积输出增加左右各一个像素, 这样就允许了1个像素的移位, 缓解了卷积层对位置的敏感性</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240814150602.png\"></p>\n<h4 id=\"平均池化层\"><a href=\"#平均池化层\" class=\"headerlink\" title=\"平均池化层\"></a>平均池化层</h4><ul>\n<li>返回滑动窗口的<strong>平均值</strong></li>\n</ul>\n<hr>\n<p><strong>总结</strong>:</p>\n<ul>\n<li>池化层返回窗口的最大或平均值</li>\n<li>缓解卷积层对位置的敏感性</li>\n<li>同样有窗口大小, 填充, 步幅作为超参数</li>\n</ul>\n<h3 id=\"批量归一化\"><a href=\"#批量归一化\" class=\"headerlink\" title=\"批量归一化\"></a>批量归一化</h3><p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240820203811.png\"></p>\n<p>损失出现在最顶端, 顶部的层训练比较快, 底部训练的很慢</p>\n<p>底部层变化时, 所以层都得跟着变, 后面的层需要重新学习很多次, 导致训练变慢</p>\n<p>因此提出:<strong>可以在学习底部层的时候避免变化顶部层吗?</strong></p>\n<p>[<a href=\"https://www.bilibili.com/video/BV12d4y1f74C\">5分钟深度学习] #06 批量归一化 Batch Normalization_哔哩哔哩_bilibili</a></p>\n<p>批量归一化根据以下表达式转换</p>\n<p>$$<br>\\mathrm{BN}(\\mathbf{x}) &#x3D; \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}.<br>$$</p>\n<p>  <strong>批量归一化层 BN层</strong></p>\n<ul>\n<li>作用在<ul>\n<li>全连接层和卷积层的输出上, 激活函数前</li>\n<li>全连接层和卷积层的输入上</li>\n</ul>\n</li>\n<li>对于全连接层, 作用在特征维</li>\n<li>对于卷积层, 作用在通道维</li>\n</ul>\n<p><strong>总结</strong></p>\n<ul>\n<li>批量归一化固定小批量中的均值和方差, 然后学习适合的偏移和缩放</li>\n<li>批评了归一化可以加快收敛速度, 但一般不改变模型精度</li>\n</ul>\n","text":"卷积神经网络卷积层 在卷积层中, 使用卷积核在输入图像上滑动, 并计算卷积核与局部图像块之间的点积, 生成特征图, 这一过程称为卷积。 如图, 3x3的卷积核在...","permalink":"/post/ML-卷积神经网络","photos":[],"count_time":{"symbolsCount":"1.7k","symbolsTime":"2 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">卷积神经网络</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%8D%B7%E7%A7%AF%E5%B1%82\"><span class=\"toc-text\">卷积层</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85\"><span class=\"toc-text\">填充和步幅</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E9%80%9A%E9%81%93\"><span class=\"toc-text\">通道</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1x1%E5%8D%B7%E7%A7%AF%E5%B1%82\"><span class=\"toc-text\">1x1卷积层</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B1%A0%E5%8C%96%E5%B1%82\"><span class=\"toc-text\">池化层</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E4%BA%8C%E7%BB%B4%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96\"><span class=\"toc-text\">二维最大池化</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96%E5%B1%82\"><span class=\"toc-text\">平均池化层</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96\"><span class=\"toc-text\">批量归一化</span></a></li></ol></li></ol>","author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"web security learner, ML learner","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"ML 现代卷积神经网络","uid":"1084723f5f84c83e372c0f57ae570bd1","slug":"ML-现代卷积神经网络","date":"2024-08-20T12:35:28.000Z","updated":"2024-08-20T14:54:45.235Z","comments":true,"path":"api/articles/ML-现代卷积神经网络.json","keywords":null,"cover":[],"text":"现代卷积神经网络LeNet AlexNetAlexNet和LeNet的设计理念和架构非常相似，但也存在显著差异。本质上来说，AlexNet是改进后的更深更大的L...","permalink":"/post/ML-现代卷积神经网络","photos":[],"count_time":{"symbolsCount":"1.9k","symbolsTime":"2 mins."},"categories":[],"tags":[],"author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"web security learner, ML learner","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"ML 模型选择","uid":"39515fa3daf39399dd3c634d18623133","slug":"ML-模型选择","date":"2024-08-11T14:33:31.000Z","updated":"2024-08-12T02:39:20.761Z","comments":true,"path":"api/articles/ML-模型选择.json","keywords":null,"cover":[],"text":"训练误差和泛化误差 训练误差: 模型在训练数据上的误差 泛化误差: 模型在新数据上的误差 验证数据集和测试数据集 验证数据集: 原来评估模型好坏 验证数据集用于...","permalink":"/post/ML-模型选择","photos":[],"count_time":{"symbolsCount":471,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"web security learner, ML learner","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}