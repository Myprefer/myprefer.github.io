{"title":"ML 理论基础","uid":"23f2ddd5bdc180cd3f2c05e6b37a137d","slug":"ML-理论基础","date":"2024-09-07T16:31:52.000Z","updated":"2024-09-07T16:32:29.053Z","comments":true,"path":"api/articles/ML-理论基础.json","keywords":null,"cover":[],"content":"<h2 id=\"理论基础\"><a href=\"#理论基础\" class=\"headerlink\" title=\"理论基础\"></a>理论基础</h2><h4 id=\"讨论前向传播和反向传播的流程？\"><a href=\"#讨论前向传播和反向传播的流程？\" class=\"headerlink\" title=\"讨论前向传播和反向传播的流程？\"></a>讨论前向传播和反向传播的流程？</h4><p><strong>前向传播</strong></p>\n<ol>\n<li>在输入层输入特征向量, 传递到隐藏层</li>\n<li>在隐藏层不断计算出每一层神经元得到的结果</li>\n<li>通过激活函数得到的输出</li>\n<li>将输出传递到下一层</li>\n<li>输出层: 在隐藏层最后的输出经过激活函数后, 得到最终的预测值</li>\n</ol>\n<p><strong>反向传播</strong></p>\n<ol>\n<li><p>通过前向传播得到网络的预测值, 利用损失函数, 计算输出层的损失</p>\n</li>\n<li><p>从输出层开始, 计算损失对输出层的梯度, 并逐层反向传播该梯度</p>\n</li>\n<li><p>其后每一层通过求导的链式法则, 利用上一层传递来的梯度, 计算当前层的损失函数对各参数的梯度</p>\n</li>\n<li><p>根据计算得到的梯度, 利用优化算法(如梯度下降), 计算并更新参数, 减少损失值</p>\n</li>\n</ol>\n<h4 id=\"解释梯度下降法-并结合梯度下降法讲讲神经网络工作的流程\"><a href=\"#解释梯度下降法-并结合梯度下降法讲讲神经网络工作的流程\" class=\"headerlink\" title=\"解释梯度下降法, 并结合梯度下降法讲讲神经网络工作的流程\"></a>解释梯度下降法, 并结合梯度下降法讲讲神经网络工作的流程</h4><p><strong>解释梯度下降法</strong></p>\n<p>梯度下降算法时一种用于最小化损失函数的优化算法, 通过调整神经网络中的参数来优化模型</p>\n<p>梯度时损失函数对参数的(导数)偏导, 直观体现为损失函数下降最快的方向, 沿着该方向调整参数可以使损失减小</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408082044259.png\"></p>\n<p>算法公式为, 其中α为学习率, J为代价函数</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202407312122596.png\"></p>\n<p><strong>神经网络工作流程</strong></p>\n<ol>\n<li>初始化参数, 如对<code>y = W * x + b</code>的W和b进行随机初始化</li>\n<li>输入数据, 进行前向传播, 将数据通过网络逐层从输入层传递到输出层</li>\n<li>在隐藏层的各层进行指定的计算并通过激活函数得到输出, 逐层前线传播</li>\n<li>在输出层得到预测值</li>\n<li>利用损失函数计算输出层的预测值的损失</li>\n<li>从输出层开始, 反向逐层计算损失函数对于各参数的梯度(反向传播)</li>\n<li>根据梯度下降公式<code>W = W - α * L&#39;(W, b)</code>和<code>b = b - α * L&#39;(W, b)</code>(<em>偏导打不出来</em>)更新参数, 其中<code>L</code>是损失函数, α是学习率</li>\n<li>重复上述的前向传播和反向传播, 进行多次迭代训练, 知道损失函数收敛到一个较小的值</li>\n</ol>\n<h4 id=\"谈谈你对损失函数的理解-并解释交叉熵损失函数的原理\"><a href=\"#谈谈你对损失函数的理解-并解释交叉熵损失函数的原理\" class=\"headerlink\" title=\"谈谈你对损失函数的理解, 并解释交叉熵损失函数的原理\"></a>谈谈你对损失函数的理解, 并解释交叉熵损失函数的原理</h4><p><strong>损失函数</strong></p>\n<p>数据在经过模型的处理后会得到一个预测值, 预测值和真实值之间往往有所差距, 这个差距越小就说明模型预测的越准确, 因此在机器学习中, 我们希望这个差距尽可能小。 损失函数就是可以体现出这个差距的函数, 在神经网络中, 损失函数接收模型的预测值, 并计算预测值与真实值的差距, 为接下来的反向传播提供数据, 进而提供更新模型参数的方向</p>\n<p><strong>交叉熵损失函数</strong></p>\n<p>在二分类问题中交叉熵损失函数为:</p>\n<div class=\"language-txt\"><button title=\"Copy code\" class=\"copy\"></button><span class=\"lang\">txt</span><pre class=\"shiki material-theme-palenight\" style=\"background-color: #1a1a1a\" tabindex=\"0\"><code><span class=\"line\"><span style=\"color: #babed8\">L = −[y*log(y^) + (1−y)*log(1−y^)]</span></span></code></pre></div><p><em><code>y^</code>为预测值</em></p>\n<p>当真实值为1时, 损失函数为<code>-log(y^)</code>, y^的值越接近1, 损失越小</p>\n<p>同样的, 当真实值为0时, 损失函数为<code>-log(1-y^)</code>, y^的值越接近0, 损失越小</p>\n<p>这样, 可以有针对性的对预测值与真实值的差距进行计算, 在二分类问题中, 一个数据的标签(即真实值)不是1就是0, 用该函数可以很好的体现预测值于真实标签的差距</p>\n<p>类似的, 在多分类问题中, 使用one-hot编码实现, 具体为:<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408092215513.png\"></p>\n<p>yk是真实标签在第k类上的值, 例如有5类, 实际为第一类, 则<code>y = [1,0,0,0,0]</code>, 这样损失函数为<code>-log(y1^)</code>, 能够准确的反映这个差距</p>\n<h4 id=\"谈谈你对激活函数的理解\"><a href=\"#谈谈你对激活函数的理解\" class=\"headerlink\" title=\"谈谈你对激活函数的理解\"></a>谈谈你对激活函数的理解</h4><p>激活函数的主要作用是引入非线性因素, 让数据进行非线性变换, 使神经网络可以拟合复杂的非线性关系</p>\n<p>如果没有用激活函数, 神经网络只会执行线性变换, 每一层的输入都是上一层输出的线性变换, 这样, 即使网络有很多层, 也只相当于一个单层线性模型, 不能处理更加复杂的关系</p>\n<p>比如, 一个隐藏层为<code>h(1) = W(1)*x + b(1)</code>, 另一个为&#96;&#96;h(2) &#x3D; W(2)*x + b(2)&#96;, 那么经过两层的计算, 结果为<br>$$<br>h(2)&#x3D;W(2)(W(1)x+b(1))+b(2)<br>    &#x3D;(W(2)W(1))x+(W(2)b(1)+b(2))<br>    &#x3D;W’x + b’<br>$$<br>仍然是线性变换, 那么这么多层的隐藏层就没有意义了, 因为一层就能处理线性组合, 这时最终的输出也仍然是线性组合</p>\n<p>如果在这两个层之间引入激活函数, 神经网络的层与层之间不再是简单的线性组合, 而是产生了更加复杂的的非线性关系</p>\n<h4 id=\"解释梯度消失和梯度爆炸-并给出你觉的有效的解决方法\"><a href=\"#解释梯度消失和梯度爆炸-并给出你觉的有效的解决方法\" class=\"headerlink\" title=\"解释梯度消失和梯度爆炸, 并给出你觉的有效的解决方法\"></a>解释梯度消失和梯度爆炸, 并给出你觉的有效的解决方法</h4><p><strong>梯度消失和梯度爆炸</strong></p>\n<p>假设一个神经网络有3个隐藏层, 其损失函数为<code>L</code>, 激活函数为<code>o</code>, 输出为<code>y</code></p>\n<p>即<br>$$<br>y^{(i)} &#x3D; o(z^{(i)})) &#x3D; o(W^{(i)}x^{(i)} + b^{(i)})<br>$$<br> 例如, 其第<code>3</code>层的输出为<br>$$<br>y^{(3)} &#x3D; o(z^{(3)})&#x3D; o(W^{(3)}h^{(3)} + b^{(3)})<br>$$<br>根据反向传播的原理, 第1层参数的梯度计算公式为<br>$$<br>\\frac{∂L}{∂W^{(1)}}&#x3D;\\frac{∂L}{∂y^{(3)}}\\frac{∂y^{(3)}}{∂z^{(3)}}\\frac{∂z^{(3)}}{∂x^{(3)}}\\frac{∂x^{(3)}}{∂z^{(2)}}\\frac{∂z^{(2)}}{∂x^{(2)}}\\frac{∂x^{(2)}}{∂z^{(1)}}\\frac{∂z^{(1)}}{∂W^{(1)}}<br>$$<br>也就是<br>$$<br>\\frac{∂L}{∂W^{(1)}}&#x3D;\\frac{∂L}{∂y^{(3)}}o’(z^{(3)})W^{(3)}o’(z^{(2)})W^{(2)}o’(z^{(1)})<br>$$<br>如果激活函数o为sigmoid函数, 其导数<code>o&#39;(x) = o(x)(1-o(x))</code>, 最大值为1&#x2F;4, 同样的, 如果使用tanh作为激活函数, 其导数也小于1</p>\n<p>如果初始的W小于1, 那么|W*o’(z)|&lt;&#x3D; 1&#x2F;4, 经过很多层的求导后, 梯度会因为指数级的下降而将变得非常小, 使参数变化非常缓慢, 引起了梯度消失问题</p>\n<p>如果梯度消失, 梯度值几乎会变成0那么不管怎么选择学习率,训练都不会有进展, 梯度消失对网络底部层尤为严重, 仅仅顶部层能训练的较好, 无法让神经网络更深</p>\n<p>同样的, 如果初始W比较大, 使|W*o’(z)| &gt; 1, 前面的层的梯度乘W*o’(z)的次数比后面的层更多, 前面的层比后面的层的梯度变化更快, 梯度会因为指数爆炸变得非常大, 引起梯度爆炸问题</p>\n<p>梯度爆炸可能会导致数据超出值域, 会让模型对学习率很敏感, 需要在训练过程中不断调整学习率</p>\n<p><strong>梯度消失和梯度爆炸的解决方法</strong></p>\n<ul>\n<li><p>使你用ReLU激活函数, relu的导数要么是1, 要么是0, 导数是1的时候就不存在梯度消失了</p>\n</li>\n<li><p>批量归一化(BN层), 使得每层的激活值的分布具有固定的均值(0)和方差(1), 使得激活输入值落在非线性函数对输入比较敏感的区域,  这样输入的小变化就会导致损失函数较大的变化, 使得让 梯度变大, 避免梯度消失问题产生</p>\n</li>\n<li><p>正则化, 梯度爆炸是由于W太大, 正则化可以防止其过大, 从而防止梯度爆炸</p>\n</li>\n<li><p>采用合适的权重初始化方法, 避免初始的权重过小或过大, 比如通过将权重初始化为从均值为0、方差为 1&#x2F;fan_avg的分布中的值(xavier初始化)</p>\n</li>\n<li><p>梯度裁剪, 强行限制梯度的大小, 防止梯度爆炸</p>\n</li>\n<li><p>使用残差网络</p>\n<p>ResNet的操作相当于<br>$$<br>y’’ &#x3D; g(f(x))变为y’’&#x3D;f(x) + g(f(x))<br>$$<br>这样其导数<br>$$<br>\\frac{∂y’’}{∂w}&#x3D;\\frac{∂y}{∂w}+\\frac{∂y’}{∂w}<br>$$<br>即使<code>y&#39;</code>的导数非常小, 加上y的导数后也不会太小</p>\n</li>\n</ul>\n<h4 id=\"解释过拟合-欠拟合-并给出你觉得有效的解决方法\"><a href=\"#解释过拟合-欠拟合-并给出你觉得有效的解决方法\" class=\"headerlink\" title=\"解释过拟合, 欠拟合, 并给出你觉得有效的解决方法\"></a>解释过拟合, 欠拟合, 并给出你觉得有效的解决方法</h4><p><strong>过拟合</strong></p>\n<p>模型过于强调拟合原始数据, 这样就忽略了预测新数据这个算法本质。对于原始数据, 该模型表现的非常好, 但是对于一个新数据, 其表现很可能会很差, 即不能很好的泛化</p>\n<p>直观表现如下<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408031146526.png\"></p>\n<p>模型过拟合的原因主要有:</p>\n<ul>\n<li>模型容量太大, 过于追求拟合所有细节</li>\n<li>训练数据数量太少</li>\n<li>训练数据存在噪声的干扰</li>\n</ul>\n<p><strong>解决过拟合的方法</strong>有:</p>\n<ul>\n<li>增加训练数据的量</li>\n<li>调整训练轮次和学习率等参数</li>\n<li>降低模型的复杂度(减小模型容量), 过拟合说明模型的拟合能力过强, 将模型简化可以弱化其拟合能力</li>\n<li>正则化(regularization), 在损失函数中添加正则项, 可以使模型的权重w保持在较小的值, 避免模型过于复杂</li>\n<li>每轮训练后, 实时保存模型, 输出模型在测试集上损失, 最后选取在测试集上损失最小的模型</li>\n</ul>\n<p><strong>欠拟合</strong></p>\n<p>模型无法很好的适应训练集, 会有较大的误差<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408031145373.png\"></p>\n<p>模型出现欠拟合的原因一般有:</p>\n<ul>\n<li>模型过于简单</li>\n<li>特征不足</li>\n<li>训练轮次太少</li>\n</ul>\n<p>如果欠拟合, 说明模型的学习能力不足, 增加训练数据的数量基本上不能解决问题</p>\n<p>因此, <strong>欠拟合的解决方法</strong>有</p>\n<ul>\n<li>提高模型的复杂度, 可以增加神经网络隐藏层的层数, 或者直接舍弃原来的算法, 用更加复杂的算法模型</li>\n<li>调整参数, 比如修改学习率, 训练轮次, 等等</li>\n<li>引入更多有用的特征</li>\n<li>降低正则化的约束, 正则化本来是防止模型过拟合, 但是如果模型欠拟合了, 那么就说明不需要这么强的约束, 可以降低正则化参数, 或者直接去除正则化项</li>\n</ul>\n<h4 id=\"讨论你对和正则化-regularization-和归一化-normallization-的认识\"><a href=\"#讨论你对和正则化-regularization-和归一化-normallization-的认识\" class=\"headerlink\" title=\"讨论你对和正则化(regularization)和归一化(normallization)的认识\"></a>讨论你对和正则化(regularization)和归一化(normallization)的认识</h4><p><strong>正则化</strong></p>\n<p>正则化是用来防止模型过拟合的方法</p>\n<p>例如, 模型为<code>h(x) = Wx + b</code>, 损失函数为<code>L = 1/2m * (∑(h(x) - y)^2)</code></p>\n<p>将代价函数修改为(L2正则化):<br>$$<br>L &#x3D; \\frac{1}{2m}(∑^m_{i&#x3D;0}(h(x^i) - y^i)^2) + \\frac{λ}{2m}∑^n_{i&#x3D;0}W^2_i<br>$$<br>即添加<code>λ*xx</code>项也就是正则化项(惩罚项), 其中λ为正则化参数</p>\n<p>进行梯度下降时, 某个参数的更新为:<br>$$<br>W_i &#x3D; W_i - lr\\frac{∂L}{∂W_i}<br>$$<br>损失函数对参数W_i的梯度为<br>$$<br>\\frac{∂L}{∂W_i} &#x3D; - \\frac{1}{m}∑^m_{j&#x3D;1}(y_j - \\hat{y_j})\\frac{∂\\hat{y_j}}{∂W_i} + λW_i<br>$$<br>则<br>$$<br>W_i&#x3D;(1-lr*λ)W_i-λ\\frac{∂L}{∂W_i}<br>$$<br>如果W_i较大, 那么梯度也较大, 其更新的步长也更大, 其被压缩的程度, 也就是惩罚力度也更大, 反之处罚力度较小</p>\n<p>正则化参数λ的大小可以调整惩罚的力度, 如果λ太小, 正则化参数的影响就太小, 达不到效果, 如果太大, 则会对所有的参数都进行很大力度的惩罚, 甚至使其都为0</p>\n<p>这样, 通过正则化, 可以防止某些参数的影响过大, 使其保持较小的值, 限制了模型的复杂性, 也就能够解决过拟合</p>\n<p><strong>归一化</strong></p>\n<p>normalization是一种特征缩放的方法, 是一种常用的数据预处理方法, 大部分是指把特征数据范围都缩放到(0, 1)或(-1, 1)之间<br>例如: 左边是未进行归一化的数据, x的范围是(0, 100), y的范围是(0,10), 右边是进行归一化后的数据, 范围都在(0, 1)之间</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408130107297.png\"></p>\n<p>为什么要归一化:</p>\n<ul>\n<li><p>以上图中的数据为例, 不同特征的数据范围可能相差很大, 导致某些特征对模型的训练产生较大的影响, 而其他的特征的影响可能会被忽略, 这样模型可能就不太准确</p>\n</li>\n<li><p>如果不进行归一化, 因为不同特征的范围相差较大, 直观来看, 数据会变”扁”, 进行梯度下降的时候要走很多的”弯路”<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408130106803.png\"></p>\n<p>而进行归一化之后, 可以少走很多弯路, 能让训练速度加快</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408130110513.png\"></p>\n</li>\n</ul>\n<h4 id=\"谈谈你对卷积神经网络的理解-并讨论对于cnn具体参数都是什么和如何选择\"><a href=\"#谈谈你对卷积神经网络的理解-并讨论对于cnn具体参数都是什么和如何选择\" class=\"headerlink\" title=\"谈谈你对卷积神经网络的理解, 并讨论对于cnn具体参数都是什么和如何选择\"></a>谈谈你对卷积神经网络的理解, 并讨论对于cnn具体参数都是什么和如何选择</h4><p>卷积神经网络是一种用来处理图像数据的深度学习模型, 其核心是通过卷积操作提取图像的局部特征, 卷积神经网络通常包括卷积层, 非线性层, 池化层, 全连接层, 这些层按照一定顺序叠加在一起, 逐步提取图像特征, 最后通过全连接层进行分类或其他任务</p>\n<p>在处理图像时, 由于图像特征的数据量太大(像素太多), 导致计算成本太高, 效率很低。</p>\n<p> 同时, 在传统的全连接神经网络中, 每个神经元都与前一层的每个神经元相连, 这会导致参数数量暴增。而通过卷积操作, 使用相同的卷积核处理整张图像, 这样不仅大幅减少了参数数量, 还保留了图像的空间结构</p>\n<p><strong>参数及选择</strong></p>\n<ul>\n<li><p><strong>learning rate 学习率</strong>, 一般啊在1e-1和1e-4之间选择, 需要手动调整</p>\n</li>\n<li><p><strong>epochs 训练轮次</strong> 一般轮次设定为几十到几百次, 不过一般不用全部训练完, 可以通过早停法提前结束</p>\n</li>\n<li><p><strong>batch_size 批量大小</strong>, 根据设备性能选择, 常用的批量大小有32, 64, 128, 256</p>\n</li>\n<li><p><strong>网络层数</strong> 根据模型任务的复杂度选择</p>\n</li>\n<li><p>正则化参数 </p>\n<ul>\n<li>dropout比例 0.2~0.5之间选择</li>\n<li>L2 正则化系数lambda 常用 1e-4 到 1e-2 之间的值</li>\n</ul>\n</li>\n<li><p><strong>kernel size 卷积核大小</strong> 通常用3x3或5x5, 较小的卷积核有助于捕捉细节, 较大的卷积核有助于获取更广泛的特征</p>\n</li>\n<li><p><strong>卷积核个数</strong> 每层卷积的卷积核个数决定了特征图的深度, 通常从 32 或 64 个卷积核开始, 随着网络的加深逐渐增加卷积核数量</p>\n</li>\n<li><p><strong>padding</strong> 填充, 在输入特征图的每一边添加一定数目的行列, 常见的选择是 “same” 填充(保持特征图尺寸不变)或 “valid” 填充(不添加填充), 根据任务需求选择合适的填充方式</p>\n</li>\n<li><p><strong>stride</strong> 步长, 卷积核经过输入特征图的采样间隔, 常用的步长是1或2, 大的步长可以减少特征图尺寸, 但是会损失信息</p>\n</li>\n<li><p><strong>pooling size</strong> 池化大小 通常设置为2x2</p>\n</li>\n</ul>\n<h4 id=\"1-1的卷积有什么作用\"><a href=\"#1-1的卷积有什么作用\" class=\"headerlink\" title=\"1*1的卷积有什么作用\"></a>1*1的卷积有什么作用</h4><ul>\n<li><p>1x1卷积核的高和宽都等于1, 意味着它不会识别空间信息, 因为他每次只看一个空间像素所以不会去识别通道中的空间信息</p>\n</li>\n<li><p>1x1卷积不识别空间模式, 只是融合不同通道的信息</p>\n</li>\n<li><p>相当于输入形状为n_h*n_w*c, 权重为c_o*c_i的全连接层<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408141427758.png\"></p>\n</li>\n</ul>\n<p><strong>作用</strong></p>\n<ul>\n<li><p>融合不同通道的信息, 可以认为是不做空间的匹配, 只是在输入层直接做输入通道和输出通道的融合, 等价于将整个输入拉成一个向量, 通道数等于特征的数量, 卷积核就相当于一个全连接层</p>\n</li>\n<li><p>1x1卷积可以在不改变输入特征图空间分辨率的情况下, 改变其通道数, 卷积层的输出的通道数由卷积核的个数决定, 通过设置1x1卷积核的数量, 可以做到升维&#x2F;降维</p>\n</li>\n</ul>\n<h4 id=\"卷积神经网络中池化的作用\"><a href=\"#卷积神经网络中池化的作用\" class=\"headerlink\" title=\"卷积神经网络中池化的作用\"></a>卷积神经网络中池化的作用</h4><ul>\n<li><p><strong>提取主要特征</strong>, 例如, 最大池化可以选择区域内的最大值, 可以帮助无论更好地捕捉边缘, 角点等特征</p>\n</li>\n<li><p>池化可以减少特征图的空间尺寸(宽和高), 降低了计算的复杂度, 提高效率</p>\n</li>\n<li><p><strong>缓解卷积层对位置的敏感性</strong>, 也就是增强<strong>位置不变性</strong>, 例如2x2最大池化将卷积输出增加左右各一个像素, 这样就允许了1个像素的移位, 缓解了卷积层对位置的敏感性</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240814150602.png\"></p>\n</li>\n</ul>\n","text":"理论基础讨论前向传播和反向传播的流程？前向传播 在输入层输入特征向量, 传递到隐藏层 在隐藏层不断计算出每一层神经元得到的结果 通过激活函数得到的输出 将输出传...","permalink":"/post/ML-理论基础","photos":[],"count_time":{"symbolsCount":"6.2k","symbolsTime":"6 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80\"><span class=\"toc-text\">理论基础</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%AE%A8%E8%AE%BA%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E6%B5%81%E7%A8%8B%EF%BC%9F\"><span class=\"toc-text\">讨论前向传播和反向传播的流程？</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%A7%A3%E9%87%8A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E5%B9%B6%E7%BB%93%E5%90%88%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E8%AE%B2%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B7%A5%E4%BD%9C%E7%9A%84%E6%B5%81%E7%A8%8B\"><span class=\"toc-text\">解释梯度下降法, 并结合梯度下降法讲讲神经网络工作的流程</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%B0%88%E8%B0%88%E4%BD%A0%E5%AF%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E7%90%86%E8%A7%A3-%E5%B9%B6%E8%A7%A3%E9%87%8A%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%8E%9F%E7%90%86\"><span class=\"toc-text\">谈谈你对损失函数的理解, 并解释交叉熵损失函数的原理</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%B0%88%E8%B0%88%E4%BD%A0%E5%AF%B9%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E7%90%86%E8%A7%A3\"><span class=\"toc-text\">谈谈你对激活函数的理解</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%A7%A3%E9%87%8A%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8-%E5%B9%B6%E7%BB%99%E5%87%BA%E4%BD%A0%E8%A7%89%E7%9A%84%E6%9C%89%E6%95%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95\"><span class=\"toc-text\">解释梯度消失和梯度爆炸, 并给出你觉的有效的解决方法</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%A7%A3%E9%87%8A%E8%BF%87%E6%8B%9F%E5%90%88-%E6%AC%A0%E6%8B%9F%E5%90%88-%E5%B9%B6%E7%BB%99%E5%87%BA%E4%BD%A0%E8%A7%89%E5%BE%97%E6%9C%89%E6%95%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95\"><span class=\"toc-text\">解释过拟合, 欠拟合, 并给出你觉得有效的解决方法</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%AE%A8%E8%AE%BA%E4%BD%A0%E5%AF%B9%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96-regularization-%E5%92%8C%E5%BD%92%E4%B8%80%E5%8C%96-normallization-%E7%9A%84%E8%AE%A4%E8%AF%86\"><span class=\"toc-text\">讨论你对和正则化(regularization)和归一化(normallization)的认识</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%B0%88%E8%B0%88%E4%BD%A0%E5%AF%B9%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%90%86%E8%A7%A3-%E5%B9%B6%E8%AE%A8%E8%AE%BA%E5%AF%B9%E4%BA%8Ecnn%E5%85%B7%E4%BD%93%E5%8F%82%E6%95%B0%E9%83%BD%E6%98%AF%E4%BB%80%E4%B9%88%E5%92%8C%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9\"><span class=\"toc-text\">谈谈你对卷积神经网络的理解, 并讨论对于cnn具体参数都是什么和如何选择</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1-1%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%9C%89%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8\"><span class=\"toc-text\">1*1的卷积有什么作用</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E6%B1%A0%E5%8C%96%E7%9A%84%E4%BD%9C%E7%94%A8\"><span class=\"toc-text\">卷积神经网络中池化的作用</span></a></li></ol></li></ol></li></ol>","author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"web security learner, ML learner","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"FastAPI项目 从构建到部署","uid":"bf51339dcad20c031f2ba03575ee26be","slug":"FastAPI项目-从构建到部署","date":"2024-09-24T09:31:51.000Z","updated":"2024-10-06T07:46:02.345Z","comments":true,"path":"api/articles/FastAPI项目-从构建到部署.json","keywords":null,"cover":[],"text":"FastAPI项目 从构建到部署项目框架基本结构FastAPI 路由, 视图 APIRouter Include_router Jinja2 中间件 cooki...","permalink":"/post/FastAPI项目-从构建到部署","photos":[],"count_time":{"symbolsCount":"7.5k","symbolsTime":"7 mins."},"categories":[],"tags":[],"author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"web security learner, ML learner","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"ML 现代卷积神经网络","uid":"1084723f5f84c83e372c0f57ae570bd1","slug":"ML-现代卷积神经网络","date":"2024-08-20T12:35:28.000Z","updated":"2024-08-20T14:54:45.235Z","comments":true,"path":"api/articles/ML-现代卷积神经网络.json","keywords":null,"cover":[],"text":"现代卷积神经网络LeNet AlexNetAlexNet和LeNet的设计理念和架构非常相似，但也存在显著差异。本质上来说，AlexNet是改进后的更深更大的L...","permalink":"/post/ML-现代卷积神经网络","photos":[],"count_time":{"symbolsCount":"1.9k","symbolsTime":"2 mins."},"categories":[],"tags":[],"author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"web security learner, ML learner","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}