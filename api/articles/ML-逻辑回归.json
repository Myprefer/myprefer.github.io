{"title":"ML 逻辑回归","uid":"447078f3f8d7cc4fd3994e9733503dbf","slug":"ML-逻辑回归","date":"2024-08-02T13:24:59.000Z","updated":"2024-08-12T02:37:53.081Z","comments":true,"path":"api/articles/ML-逻辑回归.json","keywords":null,"cover":[],"content":"<h2 id=\"逻辑回归\"><a href=\"#逻辑回归\" class=\"headerlink\" title=\"逻辑回归\"></a>逻辑回归</h2><p>逻辑回归用来解决<strong>分类</strong>问题, 如样本标签值为0或1, 则线性回归输出的值不太符合实际, 有较大的误差</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408022202719.png\"></p>\n<p>对上述数据使用逻辑回归模型(这里使用了sigmoid函数), 可以得到更加精确的结果<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408022222661.png\"></p>\n<h3 id=\"决策边界\"><a href=\"#决策边界\" class=\"headerlink\" title=\"决策边界\"></a>决策边界</h3><p>在分类问题中, 把不同类别的数据点分开的界限</p>\n<p>例如<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408022247517.png\"></p>\n<p>视情况而定, 决策边界可以是一条直线, 也可以是更加复杂的曲线</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408022249453.png\"></p>\n<h3 id=\"逻辑回归中的代价函数\"><a href=\"#逻辑回归中的代价函数\" class=\"headerlink\" title=\"逻辑回归中的代价函数\"></a>逻辑回归中的代价函数</h3><p>逻辑回归中, 代价函数不是凹函数, 在梯度下降中, 有多个局部最小值, 因此不能使用梯度下降的方法求解全局最小值</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408022310976.png\"></p>\n<h4 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h4><ul>\n<li>损失函数(loss function)是在一个训练样本的表现</li>\n<li>把所有训练样本的损失加起来得到的代价函数(cost function)才能衡量模型在整个训练集上的表现</li>\n</ul>\n<p> 逻辑回归中的损失函数<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408031024723.png\"></p>\n<p>简化后损失函数</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408031103882.png\"></p>\n<p>代价函数为</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408031104772.png\"></p>\n<h3 id=\"逻辑回归中的梯度下降\"><a href=\"#逻辑回归中的梯度下降\" class=\"headerlink\" title=\"逻辑回归中的梯度下降\"></a>逻辑回归中的梯度下降</h3><p>梯度下降公式</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408031123422.png\"></p>\n<p>推导得出逻辑回归的梯度下降公式与线性回归的几乎一模一样, 但是他们本质是不同的, 因为函数f是不同的</p>\n<h3 id=\"过拟合\"><a href=\"#过拟合\" class=\"headerlink\" title=\"过拟合\"></a>过拟合</h3><ul>\n<li><p>差拟合 : 无法很好的适应训练集, 会有较大的误差</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408031145373.png\"></p>\n</li>\n<li><p>泛化 : 指一个假设模型应用到新样本的能力, 我们追求泛化能力好的模型</p>\n<p><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408031145437.png\"></p>\n</li>\n<li><p>过拟合 : 模型过于强调拟合原始数据, 这样导致丢失了预测新数据的算法本质。对于原始数据, 该模型表现的非常好, 但是对于一个新数据, 其表现很可能会很差<br><img src=\"https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408031146526.png\"></p>\n</li>\n</ul>\n<h4 id=\"解决过拟合问题\"><a href=\"#解决过拟合问题\" class=\"headerlink\" title=\"解决过拟合问题\"></a>解决过拟合问题</h4><ul>\n<li><p>获取更多训练数据</p>\n</li>\n<li><p>舍弃部分不能帮助我们正确预测的特征(可能导致有用的特征被丢弃)</p>\n</li>\n<li><p>正则化: 保留原有特征, 减小参数大小</p>\n</li>\n</ul>\n<h4 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h4>","text":"逻辑回归逻辑回归用来解决分类问题, 如样本标签值为0或1, 则线性回归输出的值不太符合实际, 有较大的误差 对上述数据使用逻辑回归模型(这里使用了sigmoid...","permalink":"/post/ML-逻辑回归","photos":[],"count_time":{"symbolsCount":638,"symbolsTime":"1 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92\"><span class=\"toc-text\">逻辑回归</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C\"><span class=\"toc-text\">决策边界</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">逻辑回归中的代价函数</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">损失函数</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D\"><span class=\"toc-text\">逻辑回归中的梯度下降</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%BF%87%E6%8B%9F%E5%90%88\"><span class=\"toc-text\">过拟合</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98\"><span class=\"toc-text\">解决过拟合问题</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\"><span class=\"toc-text\"></span></a></li></ol></li></ol></li></ol>","author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"An ordinary learner","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"ML 正则化","uid":"31038f1c745d94422e3236cfc3b2ffbe","slug":"ML-正则化","date":"2024-08-03T06:40:12.000Z","updated":"2024-08-13T12:58:48.962Z","comments":true,"path":"api/articles/ML-正则化.json","keywords":null,"cover":[],"text":"正则化在机器学习模型中，正则化通过在模型的损失函数中添加一个正则项来实现。这些惩罚项通常会限制模型的复杂度，使模型更加简单，从而更好地泛化到新数据。 在代价函数...","permalink":"/post/ML-正则化","photos":[],"count_time":{"symbolsCount":610,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"An ordinary learner","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"ML 基本概念","uid":"40845efb0b4bb750c2cf6ebe2ecbd4fa","slug":"ML-基本概念","date":"2024-07-30T08:58:33.000Z","updated":"2024-08-12T14:05:55.713Z","comments":true,"path":"api/articles/ML-基本概念.json","keywords":null,"cover":[],"text":"监督学习监督学习是已经知道数据的label 即 txtLearns from being given &#39;right answers&#39;.从正确答案...","permalink":"/post/ML-基本概念","photos":[],"count_time":{"symbolsCount":"2.2k","symbolsTime":"2 mins."},"categories":[],"tags":[],"author":{"name":"Myprefer","slug":"blog-author","avatar":"https://gravatar.com/avatar/d195051819ce742212020a79d768fb6c?size=256&cache=1710768150791","link":"https://github.com/Myprefer","description":"An ordinary learner","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}