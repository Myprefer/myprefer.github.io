<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>ML 理论基础 | 网络安全</title><meta name="author" content="Myperfer"><meta name="copyright" content="Myperfer"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="理论基础讨论前向传播和反向传播的流程？前向传播  在输入层输入特征向量, 传递到隐藏层 在隐藏层不断计算出每一层神经元得到的结果 通过激活函数得到的输出 将输出传递到下一层 输出层: 在隐藏层最后的输出经过激活函数后, 得到最终的预测值  反向传播  通过前向传播得到网络的预测值, 利用损失函数, 计算输出层的损失  从输出层开始, 计算损失对输出层的梯度, 并逐层反向传播该梯度  其后每一层通过">
<meta property="og:type" content="article">
<meta property="og:title" content="ML 理论基础">
<meta property="og:url" content="https://myprefer.github.io/post/ML-%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80.html">
<meta property="og:site_name" content="网络安全">
<meta property="og:description" content="理论基础讨论前向传播和反向传播的流程？前向传播  在输入层输入特征向量, 传递到隐藏层 在隐藏层不断计算出每一层神经元得到的结果 通过激活函数得到的输出 将输出传递到下一层 输出层: 在隐藏层最后的输出经过激活函数后, 得到最终的预测值  反向传播  通过前向传播得到网络的预测值, 利用损失函数, 计算输出层的损失  从输出层开始, 计算损失对输出层的梯度, 并逐层反向传播该梯度  其后每一层通过">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://myprefer.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-09-07T16:31:52.000Z">
<meta property="article:modified_time" content="2024-09-07T16:32:29.053Z">
<meta property="article:author" content="Myperfer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://myprefer.github.io/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://myprefer.github.io/post/ML-%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: undefined,
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ML 理论基础',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="网络安全" type="application/atom+xml">
</head><body><div id="loading-box"><div class="triangles-wrap"><div class="triangles-eiz"></div><div class="triangles-seiz"></div><div class="triangles-sei"></div><div class="triangles-fei"></div><div class="triangles-feir"></div><div class="triangles-trei"></div><div class="triangles-dvai"></div><div class="triangles-ein"></div><div class="triangles-zero"></div></div></div><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">网络安全</span></a><a class="nav-page-title" href="/"><span class="site-name">ML 理论基础</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">ML 理论基础</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-09-07T16:31:52.000Z" title="Created 2024-09-08 00:31:52">2024-09-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-09-07T16:32:29.053Z" title="Updated 2024-09-08 00:32:29">2024-09-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><h4 id="讨论前向传播和反向传播的流程？"><a href="#讨论前向传播和反向传播的流程？" class="headerlink" title="讨论前向传播和反向传播的流程？"></a>讨论前向传播和反向传播的流程？</h4><p><strong>前向传播</strong></p>
<ol>
<li>在输入层输入特征向量, 传递到隐藏层</li>
<li>在隐藏层不断计算出每一层神经元得到的结果</li>
<li>通过激活函数得到的输出</li>
<li>将输出传递到下一层</li>
<li>输出层: 在隐藏层最后的输出经过激活函数后, 得到最终的预测值</li>
</ol>
<p><strong>反向传播</strong></p>
<ol>
<li><p>通过前向传播得到网络的预测值, 利用损失函数, 计算输出层的损失</p>
</li>
<li><p>从输出层开始, 计算损失对输出层的梯度, 并逐层反向传播该梯度</p>
</li>
<li><p>其后每一层通过求导的链式法则, 利用上一层传递来的梯度, 计算当前层的损失函数对各参数的梯度</p>
</li>
<li><p>根据计算得到的梯度, 利用优化算法(如梯度下降), 计算并更新参数, 减少损失值</p>
</li>
</ol>
<h4 id="解释梯度下降法-并结合梯度下降法讲讲神经网络工作的流程"><a href="#解释梯度下降法-并结合梯度下降法讲讲神经网络工作的流程" class="headerlink" title="解释梯度下降法, 并结合梯度下降法讲讲神经网络工作的流程"></a>解释梯度下降法, 并结合梯度下降法讲讲神经网络工作的流程</h4><p><strong>解释梯度下降法</strong></p>
<p>梯度下降算法时一种用于最小化损失函数的优化算法, 通过调整神经网络中的参数来优化模型</p>
<p>梯度时损失函数对参数的(导数)偏导, 直观体现为损失函数下降最快的方向, 沿着该方向调整参数可以使损失减小</p>
<p><img src="https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408082044259.png"></p>
<p>算法公式为, 其中α为学习率, J为代价函数</p>
<p><img src="https://raw.githubusercontent.com/Myprefer/ImageHost/main/202407312122596.png"></p>
<p><strong>神经网络工作流程</strong></p>
<ol>
<li>初始化参数, 如对<code>y = W * x + b</code>的W和b进行随机初始化</li>
<li>输入数据, 进行前向传播, 将数据通过网络逐层从输入层传递到输出层</li>
<li>在隐藏层的各层进行指定的计算并通过激活函数得到输出, 逐层前线传播</li>
<li>在输出层得到预测值</li>
<li>利用损失函数计算输出层的预测值的损失</li>
<li>从输出层开始, 反向逐层计算损失函数对于各参数的梯度(反向传播)</li>
<li>根据梯度下降公式<code>W = W - α * L&#39;(W, b)</code>和<code>b = b - α * L&#39;(W, b)</code>(<em>偏导打不出来</em>)更新参数, 其中<code>L</code>是损失函数, α是学习率</li>
<li>重复上述的前向传播和反向传播, 进行多次迭代训练, 知道损失函数收敛到一个较小的值</li>
</ol>
<h4 id="谈谈你对损失函数的理解-并解释交叉熵损失函数的原理"><a href="#谈谈你对损失函数的理解-并解释交叉熵损失函数的原理" class="headerlink" title="谈谈你对损失函数的理解, 并解释交叉熵损失函数的原理"></a>谈谈你对损失函数的理解, 并解释交叉熵损失函数的原理</h4><p><strong>损失函数</strong></p>
<p>数据在经过模型的处理后会得到一个预测值, 预测值和真实值之间往往有所差距, 这个差距越小就说明模型预测的越准确, 因此在机器学习中, 我们希望这个差距尽可能小。 损失函数就是可以体现出这个差距的函数, 在神经网络中, 损失函数接收模型的预测值, 并计算预测值与真实值的差距, 为接下来的反向传播提供数据, 进而提供更新模型参数的方向</p>
<p><strong>交叉熵损失函数</strong></p>
<p>在二分类问题中交叉熵损失函数为:</p>
<pre><code>L = −[y*log(y^) + (1−y)*log(1−y^)]
</code></pre>
<p><em><code>y^</code>为预测值</em></p>
<p>当真实值为1时, 损失函数为<code>-log(y^)</code>, y^的值越接近1, 损失越小</p>
<p>同样的, 当真实值为0时, 损失函数为<code>-log(1-y^)</code>, y^的值越接近0, 损失越小</p>
<p>这样, 可以有针对性的对预测值与真实值的差距进行计算, 在二分类问题中, 一个数据的标签(即真实值)不是1就是0, 用该函数可以很好的体现预测值于真实标签的差距</p>
<p>类似的, 在多分类问题中, 使用one-hot编码实现, 具体为:<br><img src="https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408092215513.png"></p>
<p>yk是真实标签在第k类上的值, 例如有5类, 实际为第一类, 则<code>y = [1,0,0,0,0]</code>, 这样损失函数为<code>-log(y1^)</code>, 能够准确的反映这个差距</p>
<h4 id="谈谈你对激活函数的理解"><a href="#谈谈你对激活函数的理解" class="headerlink" title="谈谈你对激活函数的理解"></a>谈谈你对激活函数的理解</h4><p>激活函数的主要作用是引入非线性因素, 让数据进行非线性变换, 使神经网络可以拟合复杂的非线性关系</p>
<p>如果没有用激活函数, 神经网络只会执行线性变换, 每一层的输入都是上一层输出的线性变换, 这样, 即使网络有很多层, 也只相当于一个单层线性模型, 不能处理更加复杂的关系</p>
<p>比如, 一个隐藏层为<code>h(1) = W(1)*x + b(1)</code>, 另一个为&#96;&#96;h(2) &#x3D; W(2)*x + b(2)&#96;, 那么经过两层的计算, 结果为<br>$$<br>h(2)&#x3D;W(2)(W(1)x+b(1))+b(2)<br>    &#x3D;(W(2)W(1))x+(W(2)b(1)+b(2))<br>    &#x3D;W’x + b’<br>$$<br>仍然是线性变换, 那么这么多层的隐藏层就没有意义了, 因为一层就能处理线性组合, 这时最终的输出也仍然是线性组合</p>
<p>如果在这两个层之间引入激活函数, 神经网络的层与层之间不再是简单的线性组合, 而是产生了更加复杂的的非线性关系</p>
<h4 id="解释梯度消失和梯度爆炸-并给出你觉的有效的解决方法"><a href="#解释梯度消失和梯度爆炸-并给出你觉的有效的解决方法" class="headerlink" title="解释梯度消失和梯度爆炸, 并给出你觉的有效的解决方法"></a>解释梯度消失和梯度爆炸, 并给出你觉的有效的解决方法</h4><p><strong>梯度消失和梯度爆炸</strong></p>
<p>假设一个神经网络有3个隐藏层, 其损失函数为<code>L</code>, 激活函数为<code>o</code>, 输出为<code>y</code></p>
<p>即<br>$$<br>y^{(i)} &#x3D; o(z^{(i)})) &#x3D; o(W^{(i)}x^{(i)} + b^{(i)})<br>$$<br> 例如, 其第<code>3</code>层的输出为<br>$$<br>y^{(3)} &#x3D; o(z^{(3)})&#x3D; o(W^{(3)}h^{(3)} + b^{(3)})<br>$$<br>根据反向传播的原理, 第1层参数的梯度计算公式为<br>$$<br>\frac{∂L}{∂W^{(1)}}&#x3D;\frac{∂L}{∂y^{(3)}}\frac{∂y^{(3)}}{∂z^{(3)}}\frac{∂z^{(3)}}{∂x^{(3)}}\frac{∂x^{(3)}}{∂z^{(2)}}\frac{∂z^{(2)}}{∂x^{(2)}}\frac{∂x^{(2)}}{∂z^{(1)}}\frac{∂z^{(1)}}{∂W^{(1)}}<br>$$<br>也就是<br>$$<br>\frac{∂L}{∂W^{(1)}}&#x3D;\frac{∂L}{∂y^{(3)}}o’(z^{(3)})W^{(3)}o’(z^{(2)})W^{(2)}o’(z^{(1)})<br>$$<br>如果激活函数o为sigmoid函数, 其导数<code>o&#39;(x) = o(x)(1-o(x))</code>, 最大值为1&#x2F;4, 同样的, 如果使用tanh作为激活函数, 其导数也小于1</p>
<p>如果初始的W小于1, 那么|W*o’(z)|&lt;&#x3D; 1&#x2F;4, 经过很多层的求导后, 梯度会因为指数级的下降而将变得非常小, 使参数变化非常缓慢, 引起了梯度消失问题</p>
<p>如果梯度消失, 梯度值几乎会变成0那么不管怎么选择学习率,训练都不会有进展, 梯度消失对网络底部层尤为严重, 仅仅顶部层能训练的较好, 无法让神经网络更深</p>
<p>同样的, 如果初始W比较大, 使|W*o’(z)| &gt; 1, 前面的层的梯度乘W*o’(z)的次数比后面的层更多, 前面的层比后面的层的梯度变化更快, 梯度会因为指数爆炸变得非常大, 引起梯度爆炸问题</p>
<p>梯度爆炸可能会导致数据超出值域, 会让模型对学习率很敏感, 需要在训练过程中不断调整学习率</p>
<p><strong>梯度消失和梯度爆炸的解决方法</strong></p>
<ul>
<li><p>使你用ReLU激活函数, relu的导数要么是1, 要么是0, 导数是1的时候就不存在梯度消失了</p>
</li>
<li><p>批量归一化(BN层), 使得每层的激活值的分布具有固定的均值(0)和方差(1), 使得激活输入值落在非线性函数对输入比较敏感的区域,  这样输入的小变化就会导致损失函数较大的变化, 使得让 梯度变大, 避免梯度消失问题产生</p>
</li>
<li><p>正则化, 梯度爆炸是由于W太大, 正则化可以防止其过大, 从而防止梯度爆炸</p>
</li>
<li><p>采用合适的权重初始化方法, 避免初始的权重过小或过大, 比如通过将权重初始化为从均值为0、方差为 1&#x2F;fan_avg的分布中的值(xavier初始化)</p>
</li>
<li><p>梯度裁剪, 强行限制梯度的大小, 防止梯度爆炸</p>
</li>
<li><p>使用残差网络</p>
<p>ResNet的操作相当于<br>$$<br>y’’ &#x3D; g(f(x))变为y’’&#x3D;f(x) + g(f(x))<br>$$<br>这样其导数<br>$$<br>\frac{∂y’’}{∂w}&#x3D;\frac{∂y}{∂w}+\frac{∂y’}{∂w}<br>$$<br>即使<code>y&#39;</code>的导数非常小, 加上y的导数后也不会太小</p>
</li>
</ul>
<h4 id="解释过拟合-欠拟合-并给出你觉得有效的解决方法"><a href="#解释过拟合-欠拟合-并给出你觉得有效的解决方法" class="headerlink" title="解释过拟合, 欠拟合, 并给出你觉得有效的解决方法"></a>解释过拟合, 欠拟合, 并给出你觉得有效的解决方法</h4><p><strong>过拟合</strong></p>
<p>模型过于强调拟合原始数据, 这样就忽略了预测新数据这个算法本质。对于原始数据, 该模型表现的非常好, 但是对于一个新数据, 其表现很可能会很差, 即不能很好的泛化</p>
<p>直观表现如下<br><img src="https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408031146526.png"></p>
<p>模型过拟合的原因主要有:</p>
<ul>
<li>模型容量太大, 过于追求拟合所有细节</li>
<li>训练数据数量太少</li>
<li>训练数据存在噪声的干扰</li>
</ul>
<p><strong>解决过拟合的方法</strong>有:</p>
<ul>
<li>增加训练数据的量</li>
<li>调整训练轮次和学习率等参数</li>
<li>降低模型的复杂度(减小模型容量), 过拟合说明模型的拟合能力过强, 将模型简化可以弱化其拟合能力</li>
<li>正则化(regularization), 在损失函数中添加正则项, 可以使模型的权重w保持在较小的值, 避免模型过于复杂</li>
<li>每轮训练后, 实时保存模型, 输出模型在测试集上损失, 最后选取在测试集上损失最小的模型</li>
</ul>
<p><strong>欠拟合</strong></p>
<p>模型无法很好的适应训练集, 会有较大的误差<br><img src="https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408031145373.png"></p>
<p>模型出现欠拟合的原因一般有:</p>
<ul>
<li>模型过于简单</li>
<li>特征不足</li>
<li>训练轮次太少</li>
</ul>
<p>如果欠拟合, 说明模型的学习能力不足, 增加训练数据的数量基本上不能解决问题</p>
<p>因此, <strong>欠拟合的解决方法</strong>有</p>
<ul>
<li>提高模型的复杂度, 可以增加神经网络隐藏层的层数, 或者直接舍弃原来的算法, 用更加复杂的算法模型</li>
<li>调整参数, 比如修改学习率, 训练轮次, 等等</li>
<li>引入更多有用的特征</li>
<li>降低正则化的约束, 正则化本来是防止模型过拟合, 但是如果模型欠拟合了, 那么就说明不需要这么强的约束, 可以降低正则化参数, 或者直接去除正则化项</li>
</ul>
<h4 id="讨论你对和正则化-regularization-和归一化-normallization-的认识"><a href="#讨论你对和正则化-regularization-和归一化-normallization-的认识" class="headerlink" title="讨论你对和正则化(regularization)和归一化(normallization)的认识"></a>讨论你对和正则化(regularization)和归一化(normallization)的认识</h4><p><strong>正则化</strong></p>
<p>正则化是用来防止模型过拟合的方法</p>
<p>例如, 模型为<code>h(x) = Wx + b</code>, 损失函数为<code>L = 1/2m * (∑(h(x) - y)^2)</code></p>
<p>将代价函数修改为(L2正则化):<br>$$<br>L &#x3D; \frac{1}{2m}(∑^m_{i&#x3D;0}(h(x^i) - y^i)^2) + \frac{λ}{2m}∑^n_{i&#x3D;0}W^2_i<br>$$<br>即添加<code>λ*xx</code>项也就是正则化项(惩罚项), 其中λ为正则化参数</p>
<p>进行梯度下降时, 某个参数的更新为:<br>$$<br>W_i &#x3D; W_i - lr\frac{∂L}{∂W_i}<br>$$<br>损失函数对参数W_i的梯度为<br>$$<br>\frac{∂L}{∂W_i} &#x3D; - \frac{1}{m}∑^m_{j&#x3D;1}(y_j - \hat{y_j})\frac{∂\hat{y_j}}{∂W_i} + λW_i<br>$$<br>则<br>$$<br>W_i&#x3D;(1-lr*λ)W_i-λ\frac{∂L}{∂W_i}<br>$$<br>如果W_i较大, 那么梯度也较大, 其更新的步长也更大, 其被压缩的程度, 也就是惩罚力度也更大, 反之处罚力度较小</p>
<p>正则化参数λ的大小可以调整惩罚的力度, 如果λ太小, 正则化参数的影响就太小, 达不到效果, 如果太大, 则会对所有的参数都进行很大力度的惩罚, 甚至使其都为0</p>
<p>这样, 通过正则化, 可以防止某些参数的影响过大, 使其保持较小的值, 限制了模型的复杂性, 也就能够解决过拟合</p>
<p><strong>归一化</strong></p>
<p>normalization是一种特征缩放的方法, 是一种常用的数据预处理方法, 大部分是指把特征数据范围都缩放到(0, 1)或(-1, 1)之间<br>例如: 左边是未进行归一化的数据, x的范围是(0, 100), y的范围是(0,10), 右边是进行归一化后的数据, 范围都在(0, 1)之间</p>
<p><img src="https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408130107297.png"></p>
<p>为什么要归一化:</p>
<ul>
<li><p>以上图中的数据为例, 不同特征的数据范围可能相差很大, 导致某些特征对模型的训练产生较大的影响, 而其他的特征的影响可能会被忽略, 这样模型可能就不太准确</p>
</li>
<li><p>如果不进行归一化, 因为不同特征的范围相差较大, 直观来看, 数据会变”扁”, 进行梯度下降的时候要走很多的”弯路”<br><img src="https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408130106803.png"></p>
<p>而进行归一化之后, 可以少走很多弯路, 能让训练速度加快</p>
<p><img src="https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408130110513.png"></p>
</li>
</ul>
<h4 id="谈谈你对卷积神经网络的理解-并讨论对于cnn具体参数都是什么和如何选择"><a href="#谈谈你对卷积神经网络的理解-并讨论对于cnn具体参数都是什么和如何选择" class="headerlink" title="谈谈你对卷积神经网络的理解, 并讨论对于cnn具体参数都是什么和如何选择"></a>谈谈你对卷积神经网络的理解, 并讨论对于cnn具体参数都是什么和如何选择</h4><p>卷积神经网络是一种用来处理图像数据的深度学习模型, 其核心是通过卷积操作提取图像的局部特征, 卷积神经网络通常包括卷积层, 非线性层, 池化层, 全连接层, 这些层按照一定顺序叠加在一起, 逐步提取图像特征, 最后通过全连接层进行分类或其他任务</p>
<p>在处理图像时, 由于图像特征的数据量太大(像素太多), 导致计算成本太高, 效率很低。</p>
<p> 同时, 在传统的全连接神经网络中, 每个神经元都与前一层的每个神经元相连, 这会导致参数数量暴增。而通过卷积操作, 使用相同的卷积核处理整张图像, 这样不仅大幅减少了参数数量, 还保留了图像的空间结构</p>
<p><strong>参数及选择</strong></p>
<ul>
<li><p><strong>learning rate 学习率</strong>, 一般啊在1e-1和1e-4之间选择, 需要手动调整</p>
</li>
<li><p><strong>epochs 训练轮次</strong> 一般轮次设定为几十到几百次, 不过一般不用全部训练完, 可以通过早停法提前结束</p>
</li>
<li><p><strong>batch_size 批量大小</strong>, 根据设备性能选择, 常用的批量大小有32, 64, 128, 256</p>
</li>
<li><p><strong>网络层数</strong> 根据模型任务的复杂度选择</p>
</li>
<li><p>正则化参数 </p>
<ul>
<li>dropout比例 0.2~0.5之间选择</li>
<li>L2 正则化系数lambda 常用 1e-4 到 1e-2 之间的值</li>
</ul>
</li>
<li><p><strong>kernel size 卷积核大小</strong> 通常用3x3或5x5, 较小的卷积核有助于捕捉细节, 较大的卷积核有助于获取更广泛的特征</p>
</li>
<li><p><strong>卷积核个数</strong> 每层卷积的卷积核个数决定了特征图的深度, 通常从 32 或 64 个卷积核开始, 随着网络的加深逐渐增加卷积核数量</p>
</li>
<li><p><strong>padding</strong> 填充, 在输入特征图的每一边添加一定数目的行列, 常见的选择是 “same” 填充(保持特征图尺寸不变)或 “valid” 填充(不添加填充), 根据任务需求选择合适的填充方式</p>
</li>
<li><p><strong>stride</strong> 步长, 卷积核经过输入特征图的采样间隔, 常用的步长是1或2, 大的步长可以减少特征图尺寸, 但是会损失信息</p>
</li>
<li><p><strong>pooling size</strong> 池化大小 通常设置为2x2</p>
</li>
</ul>
<h4 id="1-1的卷积有什么作用"><a href="#1-1的卷积有什么作用" class="headerlink" title="1*1的卷积有什么作用"></a>1*1的卷积有什么作用</h4><ul>
<li><p>1x1卷积核的高和宽都等于1, 意味着它不会识别空间信息, 因为他每次只看一个空间像素所以不会去识别通道中的空间信息</p>
</li>
<li><p>1x1卷积不识别空间模式, 只是融合不同通道的信息</p>
</li>
<li><p>相当于输入形状为n_h*n_w*c, 权重为c_o*c_i的全连接层<br><img src="https://raw.githubusercontent.com/Myprefer/ImageHost/main/202408141427758.png"></p>
</li>
</ul>
<p><strong>作用</strong></p>
<ul>
<li><p>融合不同通道的信息, 可以认为是不做空间的匹配, 只是在输入层直接做输入通道和输出通道的融合, 等价于将整个输入拉成一个向量, 通道数等于特征的数量, 卷积核就相当于一个全连接层</p>
</li>
<li><p>1x1卷积可以在不改变输入特征图空间分辨率的情况下, 改变其通道数, 卷积层的输出的通道数由卷积核的个数决定, 通过设置1x1卷积核的数量, 可以做到升维&#x2F;降维</p>
</li>
</ul>
<h4 id="卷积神经网络中池化的作用"><a href="#卷积神经网络中池化的作用" class="headerlink" title="卷积神经网络中池化的作用"></a>卷积神经网络中池化的作用</h4><ul>
<li><p><strong>提取主要特征</strong>, 例如, 最大池化可以选择区域内的最大值, 可以帮助无论更好地捕捉边缘, 角点等特征</p>
</li>
<li><p>池化可以减少特征图的空间尺寸(宽和高), 降低了计算的复杂度, 提高效率</p>
</li>
<li><p><strong>缓解卷积层对位置的敏感性</strong>, 也就是增强<strong>位置不变性</strong>, 例如2x2最大池化将卷积输出增加左右各一个像素, 这样就允许了1个像素的移位, 缓解了卷积层对位置的敏感性</p>
<p><img src="https://raw.githubusercontent.com/Myprefer/ImageHost/main/20240814150602.png"></p>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://myprefer.github.io">Myperfer</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://myprefer.github.io/post/ML-%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80.html">https://myprefer.github.io/post/ML-%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/post/FastAPI%E9%A1%B9%E7%9B%AE-%E4%BB%8E%E6%9E%84%E5%BB%BA%E5%88%B0%E9%83%A8%E7%BD%B2.html" title="FastAPI项目 从构建到部署"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">FastAPI项目 从构建到部署</div></div><div class="info-2"><div class="info-item-1">FastAPI项目 从构建到部署项目框架基本结构FastAPI  路由, 视图  APIRouter Include_router Jinja2   中间件  cookie session request, response   安全  JWT RBAC Depends Exception   第三方库  短信 支付(微信, 支付宝) 数据库(MySQL, pg) 缓存(File, Redis)    项目架构  项目基础构建事件监听application.add_event_handler(&quot;startup&quot;, startup(application)) application.add_event_handler(&quot;shutdown&quot;, stopping(application))  启动时调用startup()函数, 关闭时调用stopping()函数 startup函数: def startup(app: FastAPI) -&gt; Callable:     &quot;&quot;&quot;     FastApi...</div></div></div></a><a class="pagination-related" href="/post/ML-%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html" title="ML 现代卷积神经网络"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">ML 现代卷积神经网络</div></div><div class="info-2"><div class="info-item-1">现代卷积神经网络LeNet AlexNetAlexNet和LeNet的设计理念和架构非常相似，但也存在显著差异。本质上来说，AlexNet是改进后的更深更大的LeNet 主要改进:  全连接层的隐藏层后加入了丢弃层  sigmoid –&gt; ReLU (减缓梯度消失)  平均池化 –&gt; 最大池化  进行了数据增强    LeNet与AlexNet VGG(使用块的网络)VGG块：可看作更大更深的AlexNet VGG使用可重复的卷积块来构建深度卷积神经网络 VGG架构：  替换掉AlexNet整个卷积的架构，形成n个VGG块串在一起, 多个VGG块后接全连接层  不同的卷积块个数和超参数可以达到不同复杂度的变种(VGG16, VGG19)    vgg块的实现 def vgg_block(num_convs, in_channels, out_channels):     layers = []     for _ in range(num_convs):         layers.append(nn.Conv2d(in_channels,...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Myperfer</div><div class="author-info-description">By Myprefer</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">45</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="toc-number">1.</span> <span class="toc-text">理论基础</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A8%E8%AE%BA%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E6%B5%81%E7%A8%8B%EF%BC%9F"><span class="toc-number">1.0.1.</span> <span class="toc-text">讨论前向传播和反向传播的流程？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E5%B9%B6%E7%BB%93%E5%90%88%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E8%AE%B2%E8%AE%B2%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B7%A5%E4%BD%9C%E7%9A%84%E6%B5%81%E7%A8%8B"><span class="toc-number">1.0.2.</span> <span class="toc-text">解释梯度下降法, 并结合梯度下降法讲讲神经网络工作的流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%88%E8%B0%88%E4%BD%A0%E5%AF%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E7%90%86%E8%A7%A3-%E5%B9%B6%E8%A7%A3%E9%87%8A%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">1.0.3.</span> <span class="toc-text">谈谈你对损失函数的理解, 并解释交叉熵损失函数的原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%88%E8%B0%88%E4%BD%A0%E5%AF%B9%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E7%90%86%E8%A7%A3"><span class="toc-number">1.0.4.</span> <span class="toc-text">谈谈你对激活函数的理解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8-%E5%B9%B6%E7%BB%99%E5%87%BA%E4%BD%A0%E8%A7%89%E7%9A%84%E6%9C%89%E6%95%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-number">1.0.5.</span> <span class="toc-text">解释梯度消失和梯度爆炸, 并给出你觉的有效的解决方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%BF%87%E6%8B%9F%E5%90%88-%E6%AC%A0%E6%8B%9F%E5%90%88-%E5%B9%B6%E7%BB%99%E5%87%BA%E4%BD%A0%E8%A7%89%E5%BE%97%E6%9C%89%E6%95%88%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-number">1.0.6.</span> <span class="toc-text">解释过拟合, 欠拟合, 并给出你觉得有效的解决方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A8%E8%AE%BA%E4%BD%A0%E5%AF%B9%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96-regularization-%E5%92%8C%E5%BD%92%E4%B8%80%E5%8C%96-normallization-%E7%9A%84%E8%AE%A4%E8%AF%86"><span class="toc-number">1.0.7.</span> <span class="toc-text">讨论你对和正则化(regularization)和归一化(normallization)的认识</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%88%E8%B0%88%E4%BD%A0%E5%AF%B9%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%90%86%E8%A7%A3-%E5%B9%B6%E8%AE%A8%E8%AE%BA%E5%AF%B9%E4%BA%8Ecnn%E5%85%B7%E4%BD%93%E5%8F%82%E6%95%B0%E9%83%BD%E6%98%AF%E4%BB%80%E4%B9%88%E5%92%8C%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9"><span class="toc-number">1.0.8.</span> <span class="toc-text">谈谈你对卷积神经网络的理解, 并讨论对于cnn具体参数都是什么和如何选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%9C%89%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8"><span class="toc-number">1.0.9.</span> <span class="toc-text">1*1的卷积有什么作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E6%B1%A0%E5%8C%96%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">1.0.10.</span> <span class="toc-text">卷积神经网络中池化的作用</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/FastAPI%E9%A1%B9%E7%9B%AE-%E4%BB%8E%E6%9E%84%E5%BB%BA%E5%88%B0%E9%83%A8%E7%BD%B2.html" title="FastAPI项目 从构建到部署">FastAPI项目 从构建到部署</a><time datetime="2024-09-24T09:31:51.000Z" title="Created 2024-09-24 17:31:51">2024-09-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/ML-%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80.html" title="ML 理论基础">ML 理论基础</a><time datetime="2024-09-07T16:31:52.000Z" title="Created 2024-09-08 00:31:52">2024-09-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/ML-%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html" title="ML 现代卷积神经网络">ML 现代卷积神经网络</a><time datetime="2024-08-20T12:35:28.000Z" title="Created 2024-08-20 20:35:28">2024-08-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/ML-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html" title="ML 卷积神经网络">ML 卷积神经网络</a><time datetime="2024-08-13T15:50:25.000Z" title="Created 2024-08-13 23:50:25">2024-08-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/ML-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9.html" title="ML 模型选择">ML 模型选择</a><time datetime="2024-08-11T14:33:31.000Z" title="Created 2024-08-11 22:33:31">2024-08-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Myperfer</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>